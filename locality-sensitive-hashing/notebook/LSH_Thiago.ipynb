{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "import unidecode\n",
    "from math import sqrt\n",
    "import itertools\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from nltk.stem import *\n",
    "\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Defining Utils - Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentences, stopwords=None, stemmer=None):\n",
    "    sentences = str(sentences).lstrip()\n",
    "    tokens = str(sentences).split(' ')\n",
    "    tokens = [t for t in tokens if not (t == '')]\n",
    "    tokens = [unidecode.unidecode(t) for t in tokens]\n",
    "\n",
    "    # import code; code.interact(local=dict(globals(), **locals()))\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = get_stopwords()\n",
    "\n",
    "    if stemmer is None:\n",
    "        stemmer = get_stemmer()\n",
    "\n",
    "    tokens = [t.lower() for t in tokens]  # to lowercase\n",
    "    tokens = [remove_puctuation(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]  # remove stopwords\n",
    "    tokens = [stemmer.stem(t) for t in tokens]  # stemmify\n",
    "    tokens = [t for t in tokens if not (t == None)]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def bow2dist(bow, verbose=True):\n",
    "    '''\n",
    "    INPUT\n",
    "        bow: bag-of-words VxD numpy matrix\n",
    "    OUTPUT\n",
    "        dist: distance DxD lower triangular matrix\n",
    "    '''\n",
    "    d = bow.shape[1]\n",
    "    dist = np.zeros((d, d), dtype=np.float32)\n",
    "    for i in range(d):\n",
    "        for j in range(0, i):\n",
    "            dif = bow[:, i] - bow[:, j]\n",
    "            dist[i, j] = np.sqrt(np.dot(dif, dif))\n",
    "            if verbose:\n",
    "                sys.stdout.write('%05d,%05d:\\t%0.2f\\r' % (i, j, dist[i, j]))\n",
    "                sys.stdout.flush()\n",
    "    print('')\n",
    "    return dist\n",
    "\n",
    "\n",
    "def matrix2txt(mtrx, filename='mtrx.txt'):\n",
    "    '''\n",
    "    INPUT\n",
    "        mtrx: a generic numpy matrix ex: bow or dist\n",
    "    OUTPUT\n",
    "        matrix representation in text format\n",
    "        header: nrows ncols\n",
    "        body: matrix\n",
    "    '''\n",
    "    path = '../../locality-sensitive-hashing/datasets/' + filename\n",
    "    n_headers = mtrx.shape[1] - 2\n",
    "    headers = list(mtrx.shape) + [''] * n_headers\n",
    "    df = pd.DataFrame(data=mtrx.astype(np.int32), columns=headers, index=None)\n",
    "    df.to_csv(path, sep=' ', index=False, index_label=False)\n",
    "\n",
    "\n",
    "def word2idx2txt(word2idx, filename='word2idx.txt'):\n",
    "    '''\n",
    "    INPUT\n",
    "        word2idx: dict\n",
    "            keys:token\n",
    "            value:idx\n",
    "    OUTPUT\n",
    "        -\n",
    "    '''\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    path = '../../locality-sensitive-hashing/datasets/' + filename\n",
    "    df = pd.DataFrame.from_dict(idx2word, orient='index')\n",
    "    df.to_csv(path, sep=' ', index=True, index_label=False, header=None)\n",
    "\n",
    "\n",
    "def data2bow(data, word2idx):\n",
    "    '''\n",
    "        INPUT\n",
    "            data: a pandas.DataFrame\n",
    "                            processes column idx_description\n",
    "        OUTPUT\n",
    "            bow: bag-of-words VxD numpy matrix\n",
    "                    D: documents (idx_description)\n",
    "                    V: Vocabulary\n",
    "            example: if word w<=>idx appears 10 times on document d then\n",
    "                    bow[idx,d]=10\n",
    "    '''\n",
    "\n",
    "    nrows = data.shape[0]\n",
    "    ncols = len(word2idx)\n",
    "    bow = np.zeros((nrows, ncols), dtype=np.int32)\n",
    "    for r in range(nrows):\n",
    "        idx_desc = data.loc[r, \"idx_description\"]\n",
    "        if idx_desc == \"\":\n",
    "            continue\n",
    "        indexes = list(map(int, idx_desc.split(' ')))\n",
    "        for c in indexes:\n",
    "            bow[r, c] += 1\n",
    "\n",
    "    return bow.T\n",
    "\n",
    "def string2shingles(description, shingle_length):\n",
    "    description = description.lstrip()\n",
    "    tokens = description.split()\n",
    "    if len(tokens) < shingle_length:\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    k_shingles = []\n",
    "    for i in range(len(tokens) - shingle_length + 1):\n",
    "        k_shingles.append(\" \".join(tokens[i:i + shingle_length]))\n",
    "\n",
    "    return k_shingles\n",
    "\n",
    "def data2idx(data, word2idx, bool_shingle = True, shingle_length = 5):\n",
    "    '''\n",
    "        INPUT\n",
    "            data: pandas.DataFrame\n",
    "                        column: token_description\n",
    "            word2idx: dict\n",
    "                            keys:tokens,\n",
    "                            values:integer\n",
    "        OUTPUT\n",
    "            data: pandas.DataFrame\n",
    "                        column: token_description -> idx_description\n",
    "\n",
    "    '''\n",
    "    nrows = data.shape[0]\n",
    "    token_count = 0\n",
    "    for i in range(nrows):\n",
    "        # import code; code.interact(local=dict(globals(), **locals()))\n",
    "        if bool_shingle:\n",
    "            tokens = string2shingles(data.loc[i, 'token_description'], shingle_length = shingle_length)\n",
    "        else:\n",
    "            tokens = data.loc[i, 'token_description'].split(' ')\n",
    "        indexes = token2idx(tokens, word2idx)\n",
    "        token_count += len(indexes)\n",
    "        data.loc[i, 'token_description'] = \" \".join([str(idx) for idx in indexes])\n",
    "\n",
    "        sys.stdout.write('document:%d of %d\\tVOCAB:%d\\tWORD COUNT:%d\\t\\r' % (i, nrows, len(word2idx), token_count))\n",
    "        sys.stdout.flush()\n",
    "    data = data.rename(columns={'token_description': 'idx_description'})\n",
    "    print('')\n",
    "    return data\n",
    "\n",
    "\n",
    "def token2idx(tokens, word2idx):\n",
    "    '''\n",
    "        INPUT\n",
    "            tokens: a list of strings\n",
    "\n",
    "            word2idx: dict\n",
    "                            keys:tokens,\n",
    "                            values:integer\n",
    "        OUTPUT\n",
    "            bow: bag-of-words VxD numpy matrix\n",
    "                    D: documents (idx_description)\n",
    "                    V: Vocabulary\n",
    "\n",
    "    '''\n",
    "    nextidx = max(word2idx.values()) + 1 if len(word2idx) > 0  else 0\n",
    "    indexes = []\n",
    "    for t in tokens:\n",
    "        if not (t in word2idx):\n",
    "            word2idx[t] = nextidx\n",
    "            nextidx += 1\n",
    "        indexes.append(word2idx[t])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_stopwords(lang='portuguese'):\n",
    "    return set(_stopwords.words(lang))\n",
    "\n",
    "\n",
    "def get_stemmer(lang='portuguese'):\n",
    "    return SnowballStemmer(lang)\n",
    "\n",
    "\n",
    "def remove_puctuation(s):\n",
    "    # \t'''\n",
    "    # \t\ts is a string with punctuation; converts unicode to string which might get data loss\n",
    "    # \t\t\turl: https://stackoverflow.com/questions/23175809/typeerror-translate-takes-one-argument-2-given-python\n",
    "    # \t\t\t\t\t https://pypi.python.org/pypi/Unidecode\n",
    "    # \t\t\t\t\t https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate\n",
    "    # \t'''\n",
    "    # \t# return str(s).translate(None, string.punctuation)\n",
    "    # s = unidecode.unidecode(s) # Converts unicode s into closest ascii s, removes accents\n",
    "    # \tif s:\n",
    "    # \t\t# This uses the 3-argument version of str.maketrans\n",
    "    # \t\t# with arguments (x, y, z) where 'x' and 'y'\n",
    "    # \t\t# must be equal-length strings and characters in 'x'\n",
    "    # \t\t# are replaced by characters in 'y'. 'z'\n",
    "    # \t\t# is a string (string.punctuation here)\n",
    "    # \t\t# where each character in the string is mapped\n",
    "    # \t\t# to None\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))  # removes punctuation\n",
    "    s = s.translate(str.maketrans('', '', '\\n'))  # removes \\n\n",
    "    s = s.translate(str.maketrans('', '', '\\t'))  # removes \\t\n",
    "    s = s.translate(str.maketrans('', '', '\\r'))  # removes \\r\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1aaf67a65a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../datasets/development.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'records'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[1;32m    352\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    353\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Thiago\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 652\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "data_path = \"../datasets/development.json\"\n",
    "df = pd.read_json(data_path, orient='records')\n",
    "\n",
    "df.sample(10).head(10)\n",
    "\n",
    "stopwords = list(get_stopwords())\n",
    "\n",
    "stemmer= get_stemmer()\n",
    "\n",
    "tokenfy = lambda x : tokenizer(x, stopwords= stopwords, stemmer=stemmer)\n",
    "df['token_title'] = df['title'].transform(tokenfy)\n",
    "df.sample(10).head(10)\n",
    "\n",
    "df['token_description'] = df['description'].transform(tokenfy)\n",
    "\n",
    "word2idx={}\n",
    "\n",
    "df = data2idx(df, word2idx)\n",
    "\n",
    "bow = data2bow(df, word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utils - LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isPrime(n):\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if (n < 2) or (n % 2 == 0):\n",
    "        return False\n",
    "    return not any(n % i == 0 for i in range(3, int(sqrt(n)) + 1, 2))\n",
    "\n",
    "def nextPrime(n):\n",
    "    if isPrime(n):\n",
    "        n += 1\n",
    "    if (n % 2 == 0) and (n != 2):\n",
    "        n += 1\n",
    "    while True:\n",
    "        if isPrime(n):\n",
    "            break\n",
    "        n += 2\n",
    "    return n\n",
    "\n",
    "\n",
    "def universalHashFunction(x, k, maxvalue, next_prime):\n",
    "    a = np.random.randint(0, maxvalue ,k)\n",
    "    b = np.random.randint(0, maxvalue, k)\n",
    "    return (a*x + b) % next_prime\n",
    "\n",
    "def getSignatureMatrix(input_bow, num_permutations = 200):\n",
    "    nrows, ncols = input_bow.shape\n",
    "    idx = range(nrows)\n",
    "    sigM = np.empty((num_permutations, ncols))\n",
    "    sigM[:] = np.Inf\n",
    "    next_prime = nextPrime(nrows)\n",
    "    for p in range(nrows):\n",
    "        bool_1 = input_bow[p,] == 1\n",
    "        hashes = universalHashFunction(p, num_permutations, nrows, next_prime)\n",
    "        for c in range(ncols):\n",
    "            if bool_1[c]:\n",
    "                for r in range(num_permutations):\n",
    "                    if hashes[r] < sigM[r,c]:\n",
    "                        sigM[r,c] = hashes[r]\n",
    "        if p % 10000 == 0:\n",
    "            print(p, \"de\", nrows)\n",
    "\n",
    "    return sigM + 1\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "def findCandidates(sigM, num_bands):\n",
    "    num_bands = int(num_bands)\n",
    "    for c in np.where(sigM[0,] == np.Inf):\n",
    "        sigM = np.delete(sigM, c, 1)\n",
    "    nrows, ncols = sigM.shape\n",
    "    r = nrows / num_bands\n",
    "    print(r, \"linhas por banda, em media\")\n",
    "    idx_splits = list(split(range(int(nrows)), num_bands))\n",
    "    hashtable = dict.fromkeys(range(num_bands))\n",
    "    grupos_finais = dict.fromkeys(range(ncols))\n",
    "    dist_matrix_candidates = np.empty([ncols, ncols])\n",
    "    dist_matrix_candidates[:] = np.NaN\n",
    "    counter_groups = 0\n",
    "    for i in range(num_bands):\n",
    "        hashtable[i] = {}\n",
    "        col_sums = sum(sigM[idx_splits[i]])\n",
    "\n",
    "        for j in range(len(col_sums)):\n",
    "            s = col_sums[j]\n",
    "            if s in hashtable[i]:\n",
    "                hashtable[i][s].append(j)\n",
    "            else:\n",
    "                hashtable[i][s] = [j]\n",
    "\n",
    "        for l in list(hashtable[i].values()):\n",
    "            if len(l) > 1:\n",
    "                for pair in list(itertools.combinations(l, 2)):\n",
    "                    if np.isnan(dist_matrix_candidates[pair]):\n",
    "                        dist_matrix_candidates[pair] = sklearn.metrics.jaccard_similarity_score(sigM[:, pair[0]], sigM[:, pair[1]])\n",
    "                        if dist_matrix_candidates[pair] > 0.95:\n",
    "                            if grupos_finais[pair[0]]:\n",
    "                                grupos_finais[pair[1]] = grupos_finais[pair[0]]\n",
    "                            elif grupos_finais[pair[1]]:\n",
    "                                grupos_finais[pair[0]] = grupos_finais[pair[1]]\n",
    "                            else:\n",
    "                                grupos_finais[pair[0]] = counter_groups\n",
    "                                grupos_finais[pair[1]] = counter_groups\n",
    "                                counter_groups = counter_groups + 1\n",
    "\n",
    "    for u in list(grupos_finais.keys()):\n",
    "        if not grupos_finais[u]:\n",
    "            grupos_finais[u] = counter_groups\n",
    "            counter_groups = counter_groups + 1\n",
    "\n",
    "    return grupos_finais, dist_matrix_candidates, sigM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7af4bde90e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Calcula a matriz de signature -- cerca de 5 min\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msigM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSignatureMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Forma os grupos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrupos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_candidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindCandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bow' is not defined"
     ]
    }
   ],
   "source": [
    "# Calcula a matriz de signature -- cerca de 5 min\n",
    "sigM = getSignatureMatrix(bow)\n",
    "\n",
    "# Forma os grupos \n",
    "grupos, dist_candidates, sigM = findCandidates(sigM, num_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "import unidecode\n",
    "from math import sqrt\n",
    "import itertools\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords as _stopwords\n",
    "from nltk.stem import *\n",
    "\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Defining Utils - Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentences, stopwords=None, stemmer=None):\n",
    "    sentences = str(sentences).lstrip()\n",
    "    tokens = str(sentences).split(' ')\n",
    "    tokens = [t for t in tokens if not (t == '')]\n",
    "    tokens = [unidecode.unidecode(t) for t in tokens]\n",
    "\n",
    "    # import code; code.interact(local=dict(globals(), **locals()))\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = get_stopwords()\n",
    "\n",
    "    if stemmer is None:\n",
    "        stemmer = get_stemmer()\n",
    "\n",
    "    tokens = [t.lower() for t in tokens]  # to lowercase\n",
    "    tokens = [remove_puctuation(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]  # remove stopwords\n",
    "    tokens = [stemmer.stem(t) for t in tokens]  # stemmify\n",
    "    tokens = [t for t in tokens if not (t == None)]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def bow2dist(bow, verbose=True):\n",
    "    '''\n",
    "    INPUT\n",
    "        bow: bag-of-words VxD numpy matrix\n",
    "    OUTPUT\n",
    "        dist: distance DxD lower triangular matrix\n",
    "    '''\n",
    "    d = bow.shape[1]\n",
    "    dist = np.zeros((d, d), dtype=np.float32)\n",
    "    for i in range(d):\n",
    "        for j in range(0, i):\n",
    "            dif = bow[:, i] - bow[:, j]\n",
    "            dist[i, j] = np.sqrt(np.dot(dif, dif))\n",
    "            if verbose:\n",
    "                sys.stdout.write('%05d,%05d:\\t%0.2f\\r' % (i, j, dist[i, j]))\n",
    "                sys.stdout.flush()\n",
    "    print('')\n",
    "    return dist\n",
    "\n",
    "\n",
    "def matrix2txt(mtrx, filename='mtrx.txt'):\n",
    "    '''\n",
    "    INPUT\n",
    "        mtrx: a generic numpy matrix ex: bow or dist\n",
    "    OUTPUT\n",
    "        matrix representation in text format\n",
    "        header: nrows ncols\n",
    "        body: matrix\n",
    "    '''\n",
    "    path = '../../locality-sensitive-hashing/datasets/' + filename\n",
    "    n_headers = mtrx.shape[1] - 2\n",
    "    headers = list(mtrx.shape) + [''] * n_headers\n",
    "    df = pd.DataFrame(data=mtrx.astype(np.int32), columns=headers, index=None)\n",
    "    df.to_csv(path, sep=' ', index=False, index_label=False)\n",
    "\n",
    "\n",
    "def word2idx2txt(word2idx, filename='word2idx.txt'):\n",
    "    '''\n",
    "    INPUT\n",
    "        word2idx: dict\n",
    "            keys:token\n",
    "            value:idx\n",
    "    OUTPUT\n",
    "        -\n",
    "    '''\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    path = '../../locality-sensitive-hashing/datasets/' + filename\n",
    "    df = pd.DataFrame.from_dict(idx2word, orient='index')\n",
    "    df.to_csv(path, sep=' ', index=True, index_label=False, header=None)\n",
    "\n",
    "\n",
    "def data2bow(data, word2idx):\n",
    "    '''\n",
    "        INPUT\n",
    "            data: a pandas.DataFrame\n",
    "                            processes column idx_description\n",
    "        OUTPUT\n",
    "            bow: bag-of-words VxD numpy matrix\n",
    "                    D: documents (idx_description)\n",
    "                    V: Vocabulary\n",
    "            example: if word w<=>idx appears 10 times on document d then\n",
    "                    bow[idx,d]=10\n",
    "    '''\n",
    "\n",
    "    nrows = data.shape[0]\n",
    "    ncols = len(word2idx)\n",
    "    bow = np.zeros((nrows, ncols), dtype=np.int32)\n",
    "    for r in range(nrows):\n",
    "        idx_desc = data.loc[r, \"idx_description\"]\n",
    "        if idx_desc == \"\":\n",
    "            continue\n",
    "        indexes = list(map(int, idx_desc.split(' ')))\n",
    "        for c in indexes:\n",
    "            bow[r, c] += 1\n",
    "\n",
    "    return bow.T\n",
    "\n",
    "def string2shingles(description, shingle_length):\n",
    "    description = description.lstrip()\n",
    "    tokens = description.split()\n",
    "    if len(tokens) < shingle_length:\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    k_shingles = []\n",
    "    for i in range(len(tokens) - shingle_length + 1):\n",
    "        k_shingles.append(\" \".join(tokens[i:i + shingle_length]))\n",
    "\n",
    "    return k_shingles\n",
    "\n",
    "def data2idx(data, word2idx, bool_shingle = True, shingle_length = 5):\n",
    "    '''\n",
    "        INPUT\n",
    "            data: pandas.DataFrame\n",
    "                        column: token_description\n",
    "            word2idx: dict\n",
    "                            keys:tokens,\n",
    "                            values:integer\n",
    "        OUTPUT\n",
    "            data: pandas.DataFrame\n",
    "                        column: token_description -> idx_description\n",
    "\n",
    "    '''\n",
    "    nrows = data.shape[0]\n",
    "    token_count = 0\n",
    "    for i in range(nrows):\n",
    "        # import code; code.interact(local=dict(globals(), **locals()))\n",
    "        if bool_shingle:\n",
    "            tokens = string2shingles(data.loc[i, 'token_description'], shingle_length = shingle_length)\n",
    "        else:\n",
    "            tokens = data.loc[i, 'token_description'].split(' ')\n",
    "        indexes = token2idx(tokens, word2idx)\n",
    "        token_count += len(indexes)\n",
    "        data.loc[i, 'token_description'] = \" \".join([str(idx) for idx in indexes])\n",
    "\n",
    "        sys.stdout.write('document:%d of %d\\tVOCAB:%d\\tWORD COUNT:%d\\t\\r' % (i, nrows, len(word2idx), token_count))\n",
    "        sys.stdout.flush()\n",
    "    data = data.rename(columns={'token_description': 'idx_description'})\n",
    "    print('')\n",
    "    return data\n",
    "\n",
    "\n",
    "def token2idx(tokens, word2idx):\n",
    "    '''\n",
    "        INPUT\n",
    "            tokens: a list of strings\n",
    "\n",
    "            word2idx: dict\n",
    "                            keys:tokens,\n",
    "                            values:integer\n",
    "        OUTPUT\n",
    "            bow: bag-of-words VxD numpy matrix\n",
    "                    D: documents (idx_description)\n",
    "                    V: Vocabulary\n",
    "\n",
    "    '''\n",
    "    nextidx = max(word2idx.values()) + 1 if len(word2idx) > 0  else 0\n",
    "    indexes = []\n",
    "    for t in tokens:\n",
    "        if not (t in word2idx):\n",
    "            word2idx[t] = nextidx\n",
    "            nextidx += 1\n",
    "        indexes.append(word2idx[t])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def get_stopwords(lang='portuguese'):\n",
    "    return set(_stopwords.words(lang))\n",
    "\n",
    "\n",
    "def get_stemmer(lang='portuguese'):\n",
    "    return SnowballStemmer(lang)\n",
    "\n",
    "\n",
    "def remove_puctuation(s):\n",
    "    # \t'''\n",
    "    # \t\ts is a string with punctuation; converts unicode to string which might get data loss\n",
    "    # \t\t\turl: https://stackoverflow.com/questions/23175809/typeerror-translate-takes-one-argument-2-given-python\n",
    "    # \t\t\t\t\t https://pypi.python.org/pypi/Unidecode\n",
    "    # \t\t\t\t\t https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate\n",
    "    # \t'''\n",
    "    # \t# return str(s).translate(None, string.punctuation)\n",
    "    # s = unidecode.unidecode(s) # Converts unicode s into closest ascii s, removes accents\n",
    "    # \tif s:\n",
    "    # \t\t# This uses the 3-argument version of str.maketrans\n",
    "    # \t\t# with arguments (x, y, z) where 'x' and 'y'\n",
    "    # \t\t# must be equal-length strings and characters in 'x'\n",
    "    # \t\t# are replaced by characters in 'y'. 'z'\n",
    "    # \t\t# is a string (string.punctuation here)\n",
    "    # \t\t# where each character in the string is mapped\n",
    "    # \t\t# to None\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))  # removes punctuation\n",
    "    s = s.translate(str.maketrans('', '', '\\n'))  # removes \\n\n",
    "    s = s.translate(str.maketrans('', '', '\\t'))  # removes \\t\n",
    "    s = s.translate(str.maketrans('', '', '\\r'))  # removes \\r\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../datasets/development.json\"\n",
    "df = pd.read_json(data_path, orient='records')\n",
    "#df = pd.read_json('datasets/development.json', orient='records')\n",
    "df.sample(10).head(10)\n",
    "\n",
    "stopwords = list(get_stopwords())\n",
    "\n",
    "stemmer= get_stemmer()\n",
    "\n",
    "tokenfy = lambda x : tokenizer(x, stopwords= stopwords, stemmer=stemmer)\n",
    "df['token_title'] = df['title'].transform(tokenfy)\n",
    "df.sample(10).head(10)\n",
    "\n",
    "df['token_description'] = df['description'].transform(tokenfy)\n",
    "\n",
    "word2idx={}\n",
    "\n",
    "df = data2idx(df, word2idx)\n",
    "\n",
    "bow = data2bow(df, word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214861, 9998)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utils - LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isPrime(n):\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if (n < 2) or (n % 2 == 0):\n",
    "        return False\n",
    "    return not any(n % i == 0 for i in range(3, int(sqrt(n)) + 1, 2))\n",
    "\n",
    "def nextPrime(n):\n",
    "    if isPrime(n):\n",
    "        n += 1\n",
    "    if (n % 2 == 0) and (n != 2):\n",
    "        n += 1\n",
    "    while True:\n",
    "        if isPrime(n):\n",
    "            break\n",
    "        n += 2\n",
    "    return n\n",
    "\n",
    "\n",
    "def universalHashFunction(x, k, maxvalue, next_prime):\n",
    "    a = np.random.randint(0, maxvalue ,k)\n",
    "    b = np.random.randint(0, maxvalue, k)\n",
    "    return (a*x + b) % next_prime\n",
    "\n",
    "def getSignatureMatrix(input_bow, num_permutations = 200):\n",
    "    nrows, ncols = input_bow.shape\n",
    "    idx = range(nrows)\n",
    "    sigM = np.empty((num_permutations, ncols))\n",
    "    sigM[:] = np.Inf\n",
    "    next_prime = nextPrime(nrows)\n",
    "    for p in range(nrows):\n",
    "        bool_1 = input_bow[p,] == 1\n",
    "        hashes = universalHashFunction(p, num_permutations, nrows, next_prime)\n",
    "        for c in range(ncols):\n",
    "            if bool_1[c]:\n",
    "                for r in range(num_permutations):\n",
    "                    if hashes[r] < sigM[r,c]:\n",
    "                        sigM[r,c] = hashes[r]\n",
    "        if p % 10000 == 0:\n",
    "            print(p, \"de\", nrows)\n",
    "\n",
    "    return sigM + 1\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "def findCandidates(sigM, num_bands):\n",
    "    num_bands = int(num_bands)\n",
    "    for c in np.where(sigM[0,] == np.Inf):\n",
    "        sigM = np.delete(sigM, c, 1)\n",
    "    nrows, ncols = sigM.shape\n",
    "    r = nrows / num_bands\n",
    "    print(r, \"linhas por banda, em media\")\n",
    "    idx_splits = list(split(range(int(nrows)), num_bands))\n",
    "    hashtable = dict.fromkeys(range(num_bands))\n",
    "    grupos_finais = dict.fromkeys(range(ncols))\n",
    "    dist_matrix_candidates = np.empty([ncols, ncols])\n",
    "    dist_matrix_candidates[:] = np.NaN\n",
    "    counter_groups = 0\n",
    "    for i in range(num_bands):\n",
    "        hashtable[i] = {}\n",
    "        col_sums = sum(sigM[idx_splits[i]])\n",
    "\n",
    "        for j in range(len(col_sums)):\n",
    "            s = col_sums[j]\n",
    "            if s in hashtable[i]:\n",
    "                hashtable[i][s].append(j)\n",
    "            else:\n",
    "                hashtable[i][s] = [j]\n",
    "\n",
    "        for l in list(hashtable[i].values()):\n",
    "            if len(l) > 1:\n",
    "                for pair in list(itertools.combinations(l, 2)):\n",
    "                    if np.isnan(dist_matrix_candidates[pair]):\n",
    "                        dist_matrix_candidates[pair] = sklearn.metrics.jaccard_similarity_score(sigM[:, pair[0]], sigM[:, pair[1]])\n",
    "                        if dist_matrix_candidates[pair] > 0.95:\n",
    "                            if grupos_finais[pair[0]]:\n",
    "                                grupos_finais[pair[1]] = grupos_finais[pair[0]]\n",
    "                            elif grupos_finais[pair[1]]:\n",
    "                                grupos_finais[pair[0]] = grupos_finais[pair[1]]\n",
    "                            else:\n",
    "                                grupos_finais[pair[0]] = counter_groups\n",
    "                                grupos_finais[pair[1]] = counter_groups\n",
    "                                counter_groups = counter_groups + 1\n",
    "\n",
    "    for u in list(grupos_finais.keys()):\n",
    "        if not grupos_finais[u]:\n",
    "            grupos_finais[u] = counter_groups\n",
    "            counter_groups = counter_groups + 1\n",
    "\n",
    "    return grupos_finais, dist_matrix_candidates, sigM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calcula a signature matrix -- cerca de 5 minutos \n",
    "sigM = getSignatureMatrix(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0 linhas por banda, em media\n"
     ]
    }
   ],
   "source": [
    "# Forma os grupos \n",
    "grupos, dist_candidates, sigM = findCandidates(sigM, num_bands = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   121.,   2419.,   2419., ...,    839.,  12898.,  12898.],\n",
       "       [  5452.,   4285.,   4285., ...,  31570.,   9052.,  35959.],\n",
       "       [   344.,  20318.,  20318., ...,   2450.,   5044.,   2332.],\n",
       "       ..., \n",
       "       [  2859.,  44010.,  44010., ...,  12420.,  27221.,  11694.],\n",
       "       [ 19717.,  11631.,  11631., ...,  16827.,  14159.,    795.],\n",
       "       [  1639.,  16608.,  16608., ...,  10207.,     92.,     99.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 9993)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"grupos\" diz a qual grupo um determinado documento pertence. Ex:\n",
    "grupos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupos[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analise rapida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculando a matriz de distancias da signature matrix, e comparando a quantidade de pares com distancia 0 para a quantidade de pares com similaridade 1 via LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distSig = sklearn.metrics.pairwise.pairwise_distances(sigM.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(np.where(dist_candidates == 1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2766.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(list(np.where(distSig == 0))[0]) - distSig.shape[1])/2 # Ignora a diagonal principal e o triangulo superior!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados batem"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

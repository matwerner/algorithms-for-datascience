{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unidecode\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Fix seed\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regex for removing all non-alphabet letters or spaces. Also remove line breaks\n",
    "regex = re.compile(r'([^a-zA-Z :]|[\\t\\n\\r\\f\\v])')\n",
    "\n",
    "def preprocessing(documents):\n",
    "    for i in range(len(documents)):\n",
    "        # Remove all accents\n",
    "        documents[i] = unidecode.unidecode(documents[i])\n",
    "        \n",
    "        # Remove all non-alphabet letters or spaces\n",
    "        documents[i] = regex.sub(' ', documents[i])\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        documents[i] = ' '.join([token for token in documents[i].split(' ') if token])\n",
    "        \n",
    "        # To lower\n",
    "        documents[i] = documents[i].lower()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def universal_hash(prime, size):\n",
    "    # Check values\n",
    "    if prime < size:\n",
    "        raise ValueError(\"Prime number should be greater than size\")\n",
    "    \n",
    "    # Generate random values\n",
    "    a = np.random.randint(1, prime)\n",
    "    b = np.random.randint(1, prime)\n",
    "    \n",
    "    # Return hash function\n",
    "    return lambda x: ((a * x + b) % prime) % size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regex for finding local patterns\n",
    "lookup_local = [\"local\\s*:\\s*\", \"local\\s*de\\s*trabalho\\s*:\\s*\",\n",
    "                \"local\\s*da\\s*vaga\\s*:\\s*\", \"cidade\\s*:\\s*\",\n",
    "                \"localizada\\s*em\\s*\", \"local\\s*do\\s*trabalho\\s*:\\s*\",\n",
    "                \"localizacao\\s*:\\s*\"]\n",
    "local = re.compile(\"(\" + \"|\".join(lookup_local) + \")\", re.IGNORECASE)\n",
    "\n",
    "# Get a fix legth chuck after pattern\n",
    "fix_length = 10\n",
    "\n",
    "def parse_local(document):\n",
    "    match = local.search(document)\n",
    "    if match:\n",
    "        start, end = match.end(), match.end() + fix_length\n",
    "        return document[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formation Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regex for finding formation patterns\n",
    "lookup_formation = [\"formacao\\s*requerida\\s*:\\s*\",\"formacao\\s*desejada\\s*:\\s*\",\n",
    "                    \"escolaridade\\s*requerida\\s*:\\s*\", \"escolaridade\\s*:\\s*\",\n",
    "                    \"escolaridade\\s*desejada\\s*:\\s*\", \"formacao\\s*:\\s*\"]\n",
    "formation = re.compile(\"(\" + \"|\".join(lookup_formation) + \")\", re.IGNORECASE)\n",
    "\n",
    "# Get a fix legth chuck after pattern\n",
    "fix_length = 10\n",
    "\n",
    "def parse_formation(document):\n",
    "    match = formation.search(document)\n",
    "    if match:\n",
    "        start, end = match.end(), match.end() + fix_length\n",
    "        return document[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_shingles(documents, k):\n",
    "    # Map keeping the \"Hash\" of the shingles\n",
    "    shingle_map = {}\n",
    "    idx = 0\n",
    "    \n",
    "    # New document structure\n",
    "    doc_shingles = []\n",
    "    for i, document in enumerate(documents):\n",
    "        shingles = set()\n",
    "        # Split each document in k-shingles\n",
    "        for j in range(0, len(document) - k + 1):\n",
    "            # Get shingle\n",
    "            shingle = document[j:j+k]\n",
    "            \n",
    "            # For efficience purposes, apply hash\n",
    "            if shingle in shingle_map:\n",
    "                hashed_shingle = shingle_map[shingle]\n",
    "            else:\n",
    "                shingle_map[shingle] = idx\n",
    "                hashed_shingle = idx\n",
    "                idx += 1\n",
    "            \n",
    "            # Append to set of shingles\n",
    "            shingles.add(hashed_shingle)\n",
    "            \n",
    "        # Attribute to document\n",
    "        doc_shingles.append(shingles)\n",
    "        \n",
    "    return doc_shingles, shingle_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_min_hashing(documents, shingles_size, k, prime=2**61-1):\n",
    "    # Due to multiprocessing\n",
    "    np.random.seed()\n",
    "    \n",
    "    # Instantiate hash methods to be used as permutations\n",
    "    hash_methods = [universal_hash(prime, shingles_size)\n",
    "                    for i in range(k)]\n",
    "    \n",
    "    # Signature of each document\n",
    "    signatures = [[sys.maxsize\n",
    "                   for j in range(k)]\n",
    "                  for i in range(len(documents))]\n",
    "    \n",
    "    # Each shingle for each document just need to be computed once\n",
    "    computed = [set() for i in range(len(documents))]\n",
    "    \n",
    "    for i, document in enumerate(documents):\n",
    "        for shingle in document:\n",
    "            # Shingle already computed\n",
    "            if shingle in computed[i]:\n",
    "                continue\n",
    "            \n",
    "            # Compute hash for shingle\n",
    "            computed[i].add(shingle)\n",
    "            for j, hash_method in enumerate(hash_methods):\n",
    "                hash_value = hash_method(shingle)\n",
    "                \n",
    "                # Check if \"permutation position\" is lower\n",
    "                if hash_value < signatures[i][j]:\n",
    "                    signatures[i][j] = hash_value\n",
    "    \n",
    "    # Return signature of all documents\n",
    "    return np.array(signatures, dtype=np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_min_hashing_multiprocessing(documents, shingles_size, k, prime=2**61-1):\n",
    "    # Instantiate\n",
    "    threads = multiprocessing.cpu_count() - 1\n",
    "    pool = multiprocessing.Pool(processes=threads)\n",
    "    n, m = divmod(k, threads)    \n",
    "    \n",
    "    # Shared variable\n",
    "    manager = multiprocessing.Manager()\n",
    "    shared_documents = manager.list(documents)\n",
    "    \n",
    "    # Start threading\n",
    "    jobs = []\n",
    "    for i in range(threads):\n",
    "        batch = n + (m if i == 0 else 0)\n",
    "        job = pool.apply_async(compute_min_hashing,\n",
    "                               args = (shared_documents, shingles_size, batch, prime))\n",
    "        jobs.append(job)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Join all results\n",
    "    results = [job.get() for job in jobs]\n",
    "    return np.hstack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lsh(signatures, rows, bands, prime=2**61-1):\n",
    "    # Make n_buckets as large as possible\n",
    "    # For now, we will use \"1GB\"\n",
    "    n_buckets = int(10**9)\n",
    "    \n",
    "    # Instantiate hash methods\n",
    "    hash_methods = [universal_hash(prime, n_buckets)\n",
    "                    for i in range(bands)]\n",
    "    \n",
    "    # Buckets for all hashes\n",
    "    hash_buckets = [{} for i in range(bands)]\n",
    "    \n",
    "    for i, signature in enumerate(signatures):\n",
    "        for j in range(bands):\n",
    "            # Get mini signature\n",
    "            mini_signature = signature[j*rows:j*rows+rows]\n",
    "            \n",
    "            # \"Merge\" entries of vector\n",
    "            value = np.sum(np.power(mini_signature, 2))\n",
    "                \n",
    "            # Compute hash/bucket for the band\n",
    "            hash_value = hash_methods[j](value)\n",
    "\n",
    "            if hash_value in hash_buckets[j]:\n",
    "                hash_buckets[j][hash_value].append(i)\n",
    "            else:\n",
    "                hash_buckets[j][hash_value] = [i]\n",
    "    \n",
    "    return hash_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_candidates(hash_buckets, signatures, threshold):\n",
    "    # Get signature size - signatures y-axis\n",
    "    signature_size = signatures.shape[1]\n",
    "    \n",
    "    # Get all pairs\n",
    "    pairs = set()\n",
    "    for hash_bucket in hash_buckets:\n",
    "        for bucket, values in hash_bucket.items():\n",
    "            # Only interested in pairs\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Check if items are candidates (> threshold)\n",
    "            for i in range(0, len(values)):\n",
    "                for j in range(i + 1, len(values)):\n",
    "                    # Keep order, so we can eliminate duplicates\n",
    "                    if values[i] > values[j]:\n",
    "                        pairs.add((values[j], values[i]))\n",
    "                    else:\n",
    "                        pairs.add((values[i], values[j]))\n",
    "    \n",
    "    # Find all candidates                        \n",
    "    candidates = []\n",
    "    for pair in pairs:\n",
    "        idx, idy = pair\n",
    "        equal_values = np.sum(signatures[idx] == signatures[idy])\n",
    "        if equal_values >= threshold * signature_size:\n",
    "            candidates.append(pair)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_false_positives(documents, pairs):\n",
    "    true_positives = []\n",
    "    for pair in pairs:\n",
    "        idx, idy = pair\n",
    "        \n",
    "        # Check if same local\n",
    "        local_x, local_y = parse_local(documents[idx]), parse_local(documents[idy])\n",
    "        if local_x != local_y:\n",
    "            continue\n",
    "        \n",
    "        # Check if same formation\n",
    "        formation_x, formation_y = parse_formation(documents[idx]), parse_formation(documents[idy])\n",
    "        if formation_x != formation_y:\n",
    "            continue\n",
    "        \n",
    "        true_positives.append(pair)\n",
    "    \n",
    "    return true_positives        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert documents to duplicates's clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_clusters(candidates, n_items):\n",
    "    # Initialize clusters\n",
    "    cids = {i:i for i in range(n_items)}\n",
    "    clusters = {i:set([i]) for i in range(n_items)}\n",
    "    \n",
    "    # Fill clusters\n",
    "    for item_a, item_b in candidates:\n",
    "        # Already in the same cluster due to composition\n",
    "        if cids[item_a] == cids[item_b]:\n",
    "            continue\n",
    "            \n",
    "        # Get current clusters\n",
    "        cluster_a = clusters[cids[item_a]]\n",
    "        cluster_b = clusters[cids[item_b]]\n",
    "        \n",
    "        # Merge clusters\n",
    "        if len(cluster_a) >= len(cluster_b):\n",
    "            new_cid = cids[item_a]\n",
    "            old_cid = cids[item_b]\n",
    "        else:\n",
    "            new_cid = cids[item_b]\n",
    "            old_cid = cids[item_a]\n",
    "\n",
    "        # Update\n",
    "        for item in clusters[old_cid]:\n",
    "            cids[item] = new_cid\n",
    "        clusters[new_cid].update(clusters[old_cid])\n",
    "        del clusters[old_cid]\n",
    "        \n",
    "    return clusters, cids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../datasets/development.json\"\n",
    "dump_path = \"../datasets/cids.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "n_rows = 20\n",
    "n_bands = 5\n",
    "n_hashes = n_rows * n_bands\n",
    "threshold = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = io.open(data_path, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Go to beginning\n",
    "data_reader.seek(0)\n",
    "\n",
    "# Parse all text from json\n",
    "documents = [document['description']\n",
    "             for document in json.loads(data_reader.readline())\n",
    "             if document['description']]\n",
    "\n",
    "data_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 16.8 ms, total: 2.27 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%time parsed_documents = preprocessing(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 s, sys: 110 ms, total: 3.18 s\n",
      "Wall time: 3.2 s\n"
     ]
    }
   ],
   "source": [
    "%time documents_shingles, map_shingles = to_shingles(parsed_documents, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 769 ms, sys: 128 ms, total: 896 ms\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%time signatures = compute_min_hashing_multiprocessing(documents_shingles, len(map_shingles), n_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 642 ms, sys: 13 ms, total: 655 ms\n",
      "Wall time: 654 ms\n"
     ]
    }
   ],
   "source": [
    "%time hash_buckets = compute_lsh(signatures, n_rows, n_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.3 ms, sys: 2.96 ms, total: 102 ms\n",
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%time candidates = find_candidates(hash_buckets, signatures, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 19.5 ms, total: 1.03 s\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%time candidates = check_false_positives(documents, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 2.04 ms, total: 13 ms\n",
      "Wall time: 12.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time clusters, cids = set_clusters(candidates, len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump cluster ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open(dump_path, 'w') as fp:\n",
    "    for cid in cids.items():\n",
    "        fp.write('%s\\t%s\\n' % cid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81668"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3269"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6119:\toperador de estacionamentos simopark estacionamentos contrata: operador de estacionamento manobrista caixa requisitos: nao precisa ter experiencia deve ter cnh valida salario: r beneficios: cesta basica vale transporte uniforme e entre outros os curriculos podem ser entregues diretamente na rua barao de jaguara no informando que visualizou a vaga no vagas urgentes centro campinas sp ou\n",
      "7515:\toperador de estacionamentos simopark estacionamentos contrata: operador de estacionamento manobrista caixa requisitos: nao precisa ter experiencia deve ter cnh valida salario: r beneficios: cesta basica vale transporte uniforme e entre outros os curriculos podem ser entregues diretamente na rua barao de jaguara no informando que visualizou a vaga no vagas urgentes centro campinas sp ou\n",
      "\n",
      "911:\tsobre a vaga descricao area e especializacao profissional: informatica ti telecomunicacoes montagem e manutencao de micros nivel hierarquico: encarregado local de trabalho: itauna mg trabalhar na manutencao da parte fisica das maquinas hardware e configurar na rede de computadores impressoras e roteadores em geral auxiliar ao departamento de compra na descricao de equipamentos necessarios para compra beneficios: vale transporte horario: de segunda a sexta a definir informacoes adicionais: possuir cnh com experiencia em dirigir em grandes cidades disponibilidade para viajar para belo horizonte e sao paulo com o intuito de comprar instalar e fornecer manutencao nas maquinas nas demais escolas experiencia em montagem e configuracao de rede desejavel experiencia com servidores configuracao e monitoramento conhecimento atual sobre hardware exigencias escolaridade minima: curso tecnico habilitacao para dirigir categoria b disponibilidade para viajar disponibilidade para mudar de residencia beneficios adicionais vale transporte candidaturas para a vaga\n",
      "6748:\tsobre a vaga descricao area e especializacao profissional: informatica ti telecomunicacoes montagem e manutencao de micros nivel hierarquico: encarregado local de trabalho: itauna mg trabalhar na manutencao da parte fisica das maquinas hardware e configurar na rede de computadores impressoras e roteadores em geral auxiliar ao departamento de compra na descricao de equipamentos necessarios para compra beneficios: vale transporte horario: de segunda a sexta a definir informacoes adicionais: possuir cnh com experiencia em dirigir em grandes cidades disponibilidade para viajar para belo horizonte e sao paulo com o intuito de comprar instalar e fornecer manutencao nas maquinas nas demais escolas experiencia em montagem e configuracao de rede desejavel experiencia com servidores configuracao e monitoramento conhecimento atual sobre hardware exigencias escolaridade minima: curso tecnico habilitacao para dirigir categoria b disponibilidade para viajar disponibilidade para mudar de residencia beneficios adicionais vale transporte candidaturas para a vaga\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    idx = np.random.randint(0, len(candidates))\n",
    "    item_a = candidates[idx][0]\n",
    "    item_b = candidates[idx][1]\n",
    "    print('%s:\\t%s' % (item_a, documents[item_a]))\n",
    "    print('%s:\\t%s' % (item_b, documents[item_b]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

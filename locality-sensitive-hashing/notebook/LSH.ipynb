{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Fix seed\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regex for removing all non-alphabet letters or spaces\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "def preprocessing(documents):\n",
    "    for i in range(len(documents)):\n",
    "        # Remove all accents\n",
    "        documents[i] = unidecode.unidecode(documents[i])\n",
    "        \n",
    "        # Remove all non-alphabet letters or spaces\n",
    "        documents[i] = regex.sub(' ', documents[i])\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        documents[i] = ' '.join([token for token in documents[i].split(' ') if token])\n",
    "        \n",
    "        # To lower\n",
    "        documents[i] = documents[i].lower()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def universal_hash(prime, size):\n",
    "    # Check values\n",
    "    if prime < size:\n",
    "        raise ValueError(\"Prime number should be greater than size\")\n",
    "    \n",
    "    # Generate random values\n",
    "    a = np.random.randint(1, prime)\n",
    "    b = np.random.randint(1, prime)\n",
    "    \n",
    "    # Return hash function\n",
    "    return lambda x: ((a * x + b) % prime) % size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_shingles(documents, k, padding='#'):\n",
    "    # Map keeping the \"Hash\" of the shingles\n",
    "    shingle_map = {}\n",
    "    idx = 0\n",
    "    \n",
    "    # New document structure\n",
    "    doc_shingles = []\n",
    "    for i, document in enumerate(documents):\n",
    "        shingles = set()\n",
    "        # Split each document in k-shingles\n",
    "        for j in range(0, len(document), k):\n",
    "            # Get shingle\n",
    "            shingle = document[j:j+k]            \n",
    "            \n",
    "            # For the last Shingle, if necessary add padding\n",
    "            if j + k > len(document):\n",
    "                shingle += padding * (k - len(shingle))\n",
    "            \n",
    "            # For efficience purposes, apply hash\n",
    "            if shingle in shingle_map:\n",
    "                hashed_shingle = shingle_map[shingle]\n",
    "            else:\n",
    "                shingle_map[shingle] = idx\n",
    "                hashed_shingle = idx\n",
    "                idx += 1\n",
    "            \n",
    "            # Append to set of shingles\n",
    "            shingles.add(hashed_shingle)\n",
    "            \n",
    "        # Attribute to document\n",
    "        doc_shingles.append(shingles)\n",
    "        \n",
    "    return doc_shingles, shingle_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_min_hashing(documents, shingles_size, k, prime=2**61-1):\n",
    "    # Instantiate hash methods to be used as permutations\n",
    "    hash_methods = [universal_hash(prime, shingles_size)\n",
    "                    for i in range(k)]\n",
    "    \n",
    "    # Signature of each document\n",
    "    signatures = [[sys.maxsize\n",
    "                   for j in range(k)]\n",
    "                  for i in range(len(documents))]\n",
    "    \n",
    "    # Each shingle for each document just need to be computed once\n",
    "    computed = [set() for i in range(len(documents))]\n",
    "    \n",
    "    for i, document in enumerate(documents):\n",
    "        for shingle in document:\n",
    "            # Shingle already computed\n",
    "            if shingle in computed[i]:\n",
    "                continue\n",
    "            \n",
    "            # Compute hash for shingle\n",
    "            computed[i].add(shingle)\n",
    "            for j, hash_method in enumerate(hash_methods):\n",
    "                hash_value = hash_method(shingle)\n",
    "                \n",
    "                # Check if \"permutation position\" is lower\n",
    "                if hash_value < signatures[i][j]:\n",
    "                    signatures[i][j] = hash_value\n",
    "    \n",
    "    # Return signature of all documents\n",
    "    return signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lsh(signatures, rows, bands, threshold, prime=2**61-1):\n",
    "    # Make n_buckets as large as possible\n",
    "    # For now, we will use \"1GB\"\n",
    "    n_buckets = int(10**9 / bands)\n",
    "    \n",
    "    # Instantiate hash methods\n",
    "    hash_methods = [universal_hash(prime, n_buckets)\n",
    "                    for i in range(bands)]\n",
    "    \n",
    "    # Buckets for all hashes\n",
    "    hash_buckets = [{} for i in range(bands)]\n",
    "    \n",
    "    for i, signature in enumerate(signatures):\n",
    "        for j in range(0, len(signature), rows):\n",
    "            # Get mini signature\n",
    "            mini_signature = signature[j:j+rows]\n",
    "            \n",
    "            # \"Merge\" entries of vector\n",
    "            value = 1\n",
    "            for item in mini_signature:\n",
    "                value *= item\n",
    "                \n",
    "            # Compute hash/bucket for the band\n",
    "            for k, hash_method in enumerate(hash_methods):\n",
    "                hash_value = hash_method(value)\n",
    "                \n",
    "                if hash_value in hash_buckets[k]:\n",
    "                    hash_buckets[k][hash_value].append(i)\n",
    "                else:\n",
    "                    hash_buckets[k][hash_value] = [i]\n",
    "    \n",
    "    # Find all candidates\n",
    "    candidates = set()\n",
    "    for hash_bucket in hash_buckets:\n",
    "        for bucket, values in hash_bucket.items():\n",
    "            # Only interested in pairs\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Check if items are candidates (> threshold)\n",
    "            for i in range(0, len(values)):\n",
    "                for j in range(i + 1, len(values)):\n",
    "                    equal_values = 0\n",
    "                    for k in range(rows * bands):\n",
    "                        if signatures[values[i]][k] == signatures[values[j]][k]:\n",
    "                            equal_values += 1\n",
    "                    if equal_values >= threshold * rows * bands:\n",
    "                        # Keep order, so we can eliminate duplicates\n",
    "                        if values[i] > values[j]:\n",
    "                            candidates.add((values[j], values[i]))\n",
    "                        else:\n",
    "                            candidates.add((values[i], values[j]))\n",
    "    \n",
    "    return list(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../datasets/development.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "n_rows = 20\n",
    "n_bands = 5\n",
    "n_hashes = n_rows * n_bands\n",
    "threshold = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = io.open(data_path, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Go to beginning\n",
    "data_reader.seek(0)\n",
    "\n",
    "# Parse all text from json\n",
    "documents = [document['description']\n",
    "            for document in json.loads(data_reader.readline())]\n",
    "\n",
    "data_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_documents = preprocessing(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_shingles, map_shingles = to_shingles(parsed_documents, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signatures = compute_min_hashing(documents_shingles, len(map_shingles), n_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = compute_lsh(signatures, n_rows, n_bands, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45678"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3389"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535:\tatividades profissionais elaborar e emitir desenhos relativos aos projetos de produtos maquinas e ferramentas desenvolver desenhos em autocad solidworks e outras ferramentas elaborar fluxogramas layouts pecas para fabricacao e documentacao tecnica\n",
      "5923:\tatividades profissionais elaborar e emitir desenhos relativos aos projetos de produtos maquinas e ferramentas desenvolver desenhos em autocad solidworks e outras ferramentas elaborar fluxogramas layouts pecas para fabricacao e documentacao tecnica\n",
      "\n",
      "6010:\tatividades apoio a area comercial atender central de telefones e celulares da empresa atualizacao e confeccao de planilhas de controle apoio ao faturamento quando necessario suporte area comercial como liberacao de pedidos pesquisa no cadastro de clientes liberacao de agenda para vendedores e todas as demais funcoes de suporte horario das as empresa empresa comercial e distribuidora localizada na zona norte de porto alegre requisitos conhecimento solido na funcao disponibilidade para trabalhar no bairro sarandi e disponibilidade de horario remuneracao r beneficios vr vt unimed cartao do sesc e convenio com universidades\n",
      "7892:\tatividades apoio a area comercial atender central de telefones e celulares da empresa atualizacao e confeccao de planilhas de controle apoio ao faturamento quando necessario suporte area comercial como liberacao de pedidos pesquisa no cadastro de clientes liberacao de agenda para vendedores e todas as demais funcoes de suporte horario das as empresa empresa comercial e distribuidora localizada na zona norte de porto alegre requisitos conhecimento solido na funcao disponibilidade para trabalhar no bairro sarandi e disponibilidade de horario remuneracao r beneficios vr vt unimed cartao do sesc e convenio com universidades\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    idx = np.random.randint(0, len(candidates))\n",
    "    item_a = candidates[idx][0]\n",
    "    item_b = candidates[idx][1]\n",
    "    print('%s:\\t%s' % (item_a, documents[item_a]))\n",
    "    print('%s:\\t%s' % (item_b, documents[item_b]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

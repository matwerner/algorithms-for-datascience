{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unidecode\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Fix seed\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regex for removing all non-alphabet letters or spaces\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "def preprocessing(documents):\n",
    "    for i in range(len(documents)):\n",
    "        # Remove all accents\n",
    "        documents[i] = unidecode.unidecode(documents[i])\n",
    "        \n",
    "        # Remove all non-alphabet letters or spaces\n",
    "        documents[i] = regex.sub(' ', documents[i])\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        documents[i] = ' '.join([token for token in documents[i].split(' ') if token])\n",
    "        \n",
    "        # To lower\n",
    "        documents[i] = documents[i].lower()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def universal_hash(prime, size):\n",
    "    # Check values\n",
    "    if prime < size:\n",
    "        raise ValueError(\"Prime number should be greater than size\")\n",
    "    \n",
    "    # Generate random values\n",
    "    a = np.random.randint(1, prime)\n",
    "    b = np.random.randint(1, prime)\n",
    "    \n",
    "    # Return hash function\n",
    "    return lambda x: ((a * x + b) % prime) % size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_shingles(documents, k):\n",
    "    # Map keeping the \"Hash\" of the shingles\n",
    "    shingle_map = {}\n",
    "    idx = 0\n",
    "    \n",
    "    # New document structure\n",
    "    doc_shingles = []\n",
    "    for i, document in enumerate(documents):\n",
    "        shingles = set()\n",
    "        # Split each document in k-shingles\n",
    "        for j in range(0, len(document) - k + 1):\n",
    "            # Get shingle\n",
    "            shingle = document[j:j+k]\n",
    "            \n",
    "            # For efficience purposes, apply hash\n",
    "            if shingle in shingle_map:\n",
    "                hashed_shingle = shingle_map[shingle]\n",
    "            else:\n",
    "                shingle_map[shingle] = idx\n",
    "                hashed_shingle = idx\n",
    "                idx += 1\n",
    "            \n",
    "            # Append to set of shingles\n",
    "            shingles.add(hashed_shingle)\n",
    "            \n",
    "        # Attribute to document\n",
    "        doc_shingles.append(shingles)\n",
    "        \n",
    "    return doc_shingles, shingle_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_min_hashing(documents, shingles_size, k, prime=2**61-1):\n",
    "    # Instantiate hash methods to be used as permutations\n",
    "    hash_methods = [universal_hash(prime, shingles_size)\n",
    "                    for i in range(k)]\n",
    "    \n",
    "    # Signature of each document\n",
    "    signatures = [[sys.maxsize\n",
    "                   for j in range(k)]\n",
    "                  for i in range(len(documents))]\n",
    "    # signatures = np.full((len(documents), k), np.Inf, dtype=np.int32)\n",
    "    \n",
    "    # Each shingle for each document just need to be computed once\n",
    "    computed = [set() for i in range(len(documents))]\n",
    "    \n",
    "    for i, document in enumerate(documents):\n",
    "        for shingle in document:\n",
    "            # Shingle already computed\n",
    "            if shingle in computed[i]:\n",
    "                continue\n",
    "            \n",
    "            # Compute hash for shingle\n",
    "            computed[i].add(shingle)\n",
    "            for j, hash_method in enumerate(hash_methods):\n",
    "                hash_value = hash_method(shingle)\n",
    "                \n",
    "                # Check if \"permutation position\" is lower\n",
    "                if hash_value < signatures[i][j]:\n",
    "                    signatures[i][j] = hash_value\n",
    "    \n",
    "    # Return signature of all documents\n",
    "    return np.array(signatures, dtype=np.uint64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lsh(signatures, rows, bands, prime=2**61-1):\n",
    "    # Make n_buckets as large as possible\n",
    "    # For now, we will use \"1GB\"\n",
    "    n_buckets = int(10**9)\n",
    "    \n",
    "    # Instantiate hash methods\n",
    "    hash_methods = [universal_hash(prime, n_buckets)\n",
    "                    for i in range(bands)]\n",
    "    \n",
    "    # Buckets for all hashes\n",
    "    hash_buckets = [{} for i in range(bands)]\n",
    "    \n",
    "    for i, signature in enumerate(signatures):\n",
    "        for j in range(bands):\n",
    "            # Get mini signature\n",
    "            mini_signature = signature[j*rows:j*rows+rows]\n",
    "            \n",
    "            # \"Merge\" entries of vector\n",
    "            value = np.sum(np.power(mini_signature, 2))\n",
    "                \n",
    "            # Compute hash/bucket for the band\n",
    "            hash_value = hash_methods[j](value)\n",
    "\n",
    "            if hash_value in hash_buckets[j]:\n",
    "                hash_buckets[j][hash_value].append(i)\n",
    "            else:\n",
    "                hash_buckets[j][hash_value] = [i]\n",
    "    \n",
    "    return hash_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_candidates(hash_buckets, signatures, threshold):\n",
    "    # Get signature size - signatures y-axis\n",
    "    signature_size = signatures.shape[1]\n",
    "    \n",
    "    # Get all pairs\n",
    "    pairs = set()\n",
    "    for hash_bucket in hash_buckets:\n",
    "        for bucket, values in hash_bucket.items():\n",
    "            # Only interested in pairs\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Check if items are candidates (> threshold)\n",
    "            for i in range(0, len(values)):\n",
    "                for j in range(i + 1, len(values)):\n",
    "                    # Keep order, so we can eliminate duplicates\n",
    "                    if values[i] > values[j]:\n",
    "                        pairs.add((values[j], values[i]))\n",
    "                    else:\n",
    "                        pairs.add((values[i], values[j]))\n",
    "    \n",
    "    # Find all candidates                        \n",
    "    candidates = []\n",
    "    for pair in pairs:\n",
    "        idx, idy = pair\n",
    "        equal_values = np.sum(signatures[idx] == signatures[idy])\n",
    "        if equal_values >= threshold * signature_size:\n",
    "            candidates.append(pair)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_clusters(candidates, n_items):\n",
    "    # Initialize clusters\n",
    "    cids = {i:i for i in range(n_items)}\n",
    "    clusters = {i:set([i]) for i in range(n_items)}\n",
    "    \n",
    "    # Fill clusters\n",
    "    for item_a, item_b in candidates:\n",
    "        # Already in the same cluster due to composition\n",
    "        if cids[item_a] == cids[item_b]:\n",
    "            continue\n",
    "            \n",
    "        # Get current clusters\n",
    "        cluster_a = clusters[cids[item_a]]\n",
    "        cluster_b = clusters[cids[item_b]]\n",
    "        \n",
    "        # Merge clusters\n",
    "        if len(cluster_a) >= len(cluster_b):\n",
    "            new_cid = cids[item_a]\n",
    "            old_cid = cids[item_b]\n",
    "        else:\n",
    "            new_cid = cids[item_b]\n",
    "            old_cid = cids[item_a]\n",
    "\n",
    "        # Update\n",
    "        for item in clusters[old_cid]:\n",
    "            cids[item] = new_cid\n",
    "        clusters[new_cid].update(clusters[old_cid])\n",
    "        del clusters[old_cid]\n",
    "        \n",
    "    return clusters, cids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../datasets/development.json\"\n",
    "dump_path = \"../datasets/cids.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "n_rows = 20\n",
    "n_bands = 5\n",
    "n_hashes = n_rows * n_bands\n",
    "threshold = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = io.open(data_path, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Go to beginning\n",
    "data_reader.seek(0)\n",
    "\n",
    "# Parse all text from json\n",
    "documents = [document['description']\n",
    "             for document in json.loads(data_reader.readline())\n",
    "             if document['description']]\n",
    "\n",
    "data_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 4 ms, total: 2.23 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%time parsed_documents = preprocessing(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 s, sys: 68 ms, total: 3.4 s\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%time documents_shingles, map_shingles = to_shingles(parsed_documents, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 48s, sys: 136 ms, total: 5min 48s\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%time signatures = compute_min_hashing(documents_shingles, len(map_shingles), n_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 4 ms, total: 1.12 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%time hash_buckets = compute_lsh(signatures, n_rows, n_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 4 ms, total: 124 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%time candidates = find_candidates(hash_buckets, signatures, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 20.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time clusters, cids = set_clusters(candidates, len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump cluster ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(dump_path, 'w') as fp:\n",
    "    for cid in cids.items():\n",
    "        fp.write('%s\\t%s\\n' % cid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77050"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3305"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223:\tvaga encarregado pavimentacao paulinia spliderar equipe acompanhar e checar as escavacoes em execucao analisar a umidade e qualidade de materiais para compactacao do solo coordenar a compactacao requisitos residir em campinas hortolndia sumare paulinia experiencia comprovada na funcao beneficios vale transporte ou auxilio combustivel refeicao no local cesta basica convenio medico e odontologico seguro de vida horario de trabalho de a a a das as horas\n",
      "4679:\tvaga encarregado pavimentacao paulinia spliderar equipe acompanhar e checar as escavacoes em execucao analisar a umidade e qualidade de materiais para compactacao do solo coordenar a compactacao requisitos residir em campinas hortolndia sumare paulinia experiencia comprovada na funcao beneficios vale transporte ou auxilio combustivel refeicao no local cesta basica convenio medico e odontologico seguro de vida horario de trabalho de a a a das as horas\n",
      "\n",
      "1223:\tvaga encarregado pavimentacao paulinia spliderar equipe acompanhar e checar as escavacoes em execucao analisar a umidade e qualidade de materiais para compactacao do solo coordenar a compactacao requisitos residir em campinas hortolndia sumare paulinia experiencia comprovada na funcao beneficios vale transporte ou auxilio combustivel refeicao no local cesta basica convenio medico e odontologico seguro de vida horario de trabalho de a a a das as horas\n",
      "1631:\tvaga encarregado pavimentacao paulinia spliderar equipe acompanhar e checar as escavacoes em execucao analisar a umidade e qualidade de materiais para compactacao do solo coordenar a compactacao requisitos residir em campinas hortolndia sumare paulinia experiencia comprovada na funcao beneficios vale transporte ou auxilio combustivel refeicao no local cesta basica convenio medico e odontologico seguro de vida horario de trabalho de a a a das as horas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    idx = np.random.randint(0, len(candidates))\n",
    "    item_a = candidates[idx][0]\n",
    "    item_b = candidates[idx][1]\n",
    "    print('%s:\\t%s' % (item_a, documents[item_a]))\n",
    "    print('%s:\\t%s' % (item_b, documents[item_b]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

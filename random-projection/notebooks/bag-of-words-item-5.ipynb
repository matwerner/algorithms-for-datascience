{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from brown import get_documents\n",
    "from g1 import get_documents\n",
    "from util import documents2bag_of_words\n",
    "from sparse import SparseVector\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article:4000 of 4000\tV:94718\tWORD COUNT:1797463\tdocid:apos-100-dias-veja-como-andam-propostas-do-prefeito-de-campinasndo-turnotoralirceuhtmlo-lula.ghtml-pauloar-corpos.ghtml\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "translate() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aada0b176683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# BoW, word2idx=  documents2bag_of_words(brown_documents)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mg1_documents\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mBoW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mdocuments2bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg1_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'portuguese'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/guilhermevarela/wrk/py/gv/algorithms-for-datascience/random-projection/notebooks/util.py\u001b[0m in \u001b[0;36mdocuments2bag_of_words\u001b[0;34m(documents, tokenize, verbose, exclude_links, lang)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mindexed_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences2indexed_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/guilhermevarela/wrk/py/gv/algorithms-for-datascience/random-projection/notebooks/util.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(l, stopwords, exclude_matcher)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m                 \u001b[0;31m# to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_puctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/guilhermevarela/wrk/py/gv/algorithms-for-datascience/random-projection/notebooks/util.py\u001b[0m in \u001b[0;36mremove_puctuation\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    112\u001b[0m \t'''\t\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# return str(s).translate(None, string.punctuation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentences2indexed_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: translate() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "# brown_documents= get_documents(verbose=True)\n",
    "# BoW, word2idx=  documents2bag_of_words(brown_documents)\n",
    "g1_documents= get_documents(verbose=True)\n",
    "BoW, word2idx=  documents2bag_of_words(g1_documents, lang='portuguese', exclude_links=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_files = BoW.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparseBoW = []\n",
    "for i in range(BoW.shape[1]):\n",
    "#     sparse_vector = sparse.SparseVector(BoW[:,i].tolist())\n",
    "    sparse_vector = SparseVector(BoW[:,i].tolist())\n",
    "    SparseBoW.append(sparse_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296.44066596\n"
     ]
    }
   ],
   "source": [
    "distances_matrix = [[0 for x in range(0, len_files)] for y in range(0, len_files)]\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(0,len_files):\n",
    "    for j in range(i+1,len_files):\n",
    "        d =  SparseBoW[i].squared_euclidian_distance(SparseBoW[j])\n",
    "        distances_matrix[i][j] = d \n",
    "        distances_matrix[j][i] = d \n",
    "        \n",
    "elapsed = timeit.default_timer() - start_time \n",
    "\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538747\n"
     ]
    }
   ],
   "source": [
    "print(BoW.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#item 6=====================================================================================================================#\n",
    "\n",
    "import math\n",
    "import timeit\n",
    "\n",
    "len_files = BoW.shape[1]\n",
    "len_dict = BoW.shape[0]\n",
    "\n",
    "def distance(i,j,BoW):\n",
    "    internal_summation = 0\n",
    "    \n",
    "    for word_index in range(0,len_dict):\n",
    "        coordinates_dif = BoW[word_index,i] - BoW[word_index,j]\n",
    "        internal_summation = internal_summation + coordinates_dif * coordinates_dif\n",
    "    \n",
    "    return internal_summation\n",
    "    \n",
    "\n",
    "distances_matrix = [[0 for x in range(0, len_files)] for y in range(0, len_files)]\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(0,len_files):\n",
    "    for j in range(i+1,len_files):\n",
    "\n",
    "        d =  distance(i,j,BoW)\n",
    "        distances_matrix[i][j] = d \n",
    "        distances_matrix[j][i] = d \n",
    "        \n",
    "elapsed = timeit.default_timer() - start_time \n",
    "\n",
    "print elapsed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
